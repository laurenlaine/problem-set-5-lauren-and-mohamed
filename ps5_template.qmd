---
title: "Problem Set 5"
author: "Lauren Laine and Mohamed Mohamed"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Lauren Laine, llaine
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup
url = 'https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
soup.text[0:50]
```

```{python}
# scrape title of the enforcement action 
# used ChatGPT to figure out the the class attribute is written as class_
#Prompt why won't this code run : soup.find_all('h2', class ='usa-card__heading')
usa_card__heading=soup.find_all('h2', class_ ='usa-card__heading')
a_tags=[]
for tag in usa_card__heading:
  a=tag.find('a').text
  a_tags.append(a)
#check and make sure all titles were collected
print(a_tags[19]) 
```

```{python}
#scrape date
dates=[]
soup_dates=soup.find_all('span', class_='text-base-dark padding-right-105')
for tag in soup_dates:
  text=tag.text
  dates.append(text)
print(dates[0:5])
print(dates[19])
```

```{python}
# scrape category
category=[]
soup_category=soup.find_all('li', class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1")
for tag in soup_category:
  text=tag.text
  category.append(text)
print(category[19])
```

```{python}
#scrape link associated with the enforecment action 
hrefs=[]
link_tags=[]
full_links=[]
for tag in usa_card__heading:
  link_tags.append(tag.find('a').attrs)

for link in link_tags:
  href=link.get('href')
  hrefs.append(href)

print(hrefs[19])
prefix='https://oig.hhs.gov/'
for href in hrefs:
  link= prefix+href
  full_links.append(link)

print(full_links)
```

```{python}
#create dataframe
df=pd.DataFrame({'Title':a_tags, 'Date':dates, 'Category':category, 'Link':full_links})

df.head()
len(df)
```
### 2. Crawling (PARTNER 1)
```{python}
url = 'https://oig.hhs.gov/fraud/enforcement/washington-doctor-settles-allegations-he-submitted-false-claims-to-federal-health-care-programs/'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
ul_tag=soup.find('ul', class_="usa-list usa-list--unstyled margin-y-2")
li_list=ul_tag.find_all('li')
print(li_list)
print(li_list[1])
#used ChatGPT to figure out how to remove the span tag. 
# Prompt remove span tag and content with Beautiful Soup in Python 
span_tag = li_list[1].find('span', class_='padding-right-2 text-base')
if span_tag:
    span_tag.decompose()
print(li_list[1].text)
```
```{python}
agencies=[]
for i in range(len(full_links)):
  url = full_links[i]
  response = requests.get(url)
  soup = BeautifulSoup(response.text, 'lxml')
  ul_tag=soup.find('ul', class_="usa-list usa-list--unstyled margin-y-2")
  li_list=ul_tag.find_all('li')
  span_tag = li_list[1].find('span', class_='padding-right-2 text-base')
  if span_tag:
    span_tag.decompose()
  agency=(li_list[1].text)
  print(agency)
  agencies.append(agency)
```
```{python}
df['Agency']=agencies
df.head()
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
import requests
from bs4 import BeautifulSoup
from datetime import datetime
```

```{python}
def check_input(year):
    if year < 2013:
        print("Please enter a year >= 2013, as only enforcement actions after 2013 are available.")
        return False
    return True

```

```{python}
def initialize_data_containers():
  return [], [], [], []
```

```{python}
def get_today_date():
    now = datetime.now()
    return now.year, now.month
```

```{python}
def get_page_content(url):
    response = requests.get(url)
    return BeautifulSoup(response.text, 'lxml')
```

```{python}
def get_titles(soup):
    titles = []
    headings = soup.find_all('h2', class_='usa-card__heading')
    for tag in headings:
        titles.append(tag.find('a').text)
    return titles
```

```{python}
def get_dates(soup):
    dates = []
    date_tags = soup.find_all('span', class_='text-base-dark padding-right-105')
    for tag in date_tags:
        dates.append(tag.text)
    return dates
```

```{python}
def get_categories(soup):
    categories = []
    category_tags = soup.find_all('li', class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1")
    for tag in category_tags:
        categories.append(tag.text)
    return categories
```

```{python}
def get_links(soup):
    full_links = []
    link_tags = [tag.find('a').attrs for tag in soup.find_all('h2', class_='usa-card__heading')]
    prefix = 'https://oig.hhs.gov/'
    for link in link_tags:
        full_links.append(prefix + link.get('href'))
    return full_links
```

```{python}
def making_dataframe(titles, dates, categories, links):
    return pd.DataFrame({
        'Title': titles,
        'Date': dates,
        'Category': categories,
        'Link': links
    })
```

```{python}
def save_to_csv(df, start_year, start_month):
    filename = f"enforcement_actions_{start_year}_{start_month:02d}.csv"
    df.to_csv(filename, index=False)
    print(f"Data saved to {filename}")
```

```{python}
def showing_summary(df):
    print(f"Number of enforcement actions: {len(df)}")
    if not df.empty:
        earliest_date = df['Date'].min()
        earliest_action = df[df['Date'] == earliest_date].iloc[0]
        print(f"Earliest enforcement action Date - {earliest_action['Date']}, Title - {earliest_action['Title']}")
```


```{python}
visited_urls = []

def scrape_enforcement_actions(start_year, start_month):
    if not check_input(start_year):
        return

    titles, dates, categories, full_links = initialize_data_containers()
    
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    
    current_year, current_month = get_today_date()
    
    
    year, month = start_year, start_month
    
    while (year < current_year) or (year == current_year and month <= current_month):
        
        url = f"{base_url}?year={year}&month={month}"
        print(f"Scraping data for {year}-{month:02d} from {url}")
        
        if url in visited_urls:
            print("Stopped because ended up in a loop.")
            break

        soup = get_page_content(url)
        
        titles.extend(get_titles(soup))
        dates.extend(get_dates(soup))
        categories.extend(get_categories(soup))
        full_links.extend(get_links(soup))

        visited_urls.append(url)
        
        time.sleep(2)  
        
        month += 1
        if month > 12:
            month = 1
            year += 1
    
    df = making_dataframe(titles, dates, categories, full_links)
    
    save_to_csv(df, start_year, start_month)
    showing_summary(df)
    
    return df
```

```{python}
# used ChatGPT to debug function and learn about concurrrent.futures.
#prompt: "Is there a way to fake the function faster"

from concurrent.futures import ThreadPoolExecutor

def scrape_agency_data(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    ul_tag = soup.find('ul', class_="usa-list usa-list--unstyled margin-y-2")
    
    if ul_tag:
        li_list = ul_tag.find_all('li')
        span_tag = li_list[1].find('span', class_='padding-right-2 text-base')
        if span_tag:
            span_tag.decompose()
        return li_list[1].text.strip()  # Return the agency
    return None


def scrape(year, month):
    date_string = f'{year}-{month+1:02d}'
    set_date = pd.to_datetime(date_string, format='%Y-%m')

    titles = []
    dates = []
    categories = []
    full_links = []
    agencies = []

    session = requests.Session()

    for i in range(250):
        base = 'https://oig.hhs.gov/fraud/enforcement/?page='
        url = f'{base}{i}'
        response = session.get(url)
        soup = BeautifulSoup(response.text, 'lxml')

        body = soup.find('body')
        soup_dates = body.find_all('span', class_='text-base-dark padding-right-105')
        soup_titles = body.find_all('h2', class_='usa-card__heading')
        soup_category = body.find_all('li', class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1")


        for date_tag, title_tag, category_tag in zip(soup_dates, soup_titles, soup_category):
            date = pd.to_datetime(date_tag.text, format='%B %d, %Y')
            dates.append(date)
            title = title_tag.find('a').text
            titles.append(title)
            category = category_tag.text.strip()
            categories.append(category)
            href = title_tag.find('a').attrs.get('href')
            full_link = f'https://oig.hhs.gov/{href}'
            full_links.append(full_link)


        if date < set_date:
            break

    # Use ThreadPoolExecutor to scrape agency data in parallel
    with ThreadPoolExecutor() as executor:
        agencies = list(executor.map(scrape_agency_data, full_links))


    scraped_data = pd.DataFrame({
        'Title': titles,
        'Date': dates,
        'Category': categories,
        'Link': full_links,
        'Agency': agencies
    })
    
    return scraped_data
```
```{python}
# Running scraper starting from January 2023
scraped = scrape(2023, 1)
```

```{python}
print(len(scraped))
print(scraped.tail(1))
```

* c. Test Partner's Code (PARTNER 1)

```{python}
df_21=scrape(2021, 1 )
```

```{python}
print(len(df_21))
print(df_21.tail(1))

```
The length of the dataframe is 3020. 

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
filtered_df_21 = df_21[(df_21['Category'] == "Criminal and Civil Actions") | (df_21['Category'] == 'State Enforcement Agencies')]

filtered_df_21.head(20)

```
```{python}
actions_by_category=alt.Chart(filtered_df_21, title='Number of Enforcement Actions by Category over Time').mark_line().encode(
  alt.X('year(Date)'),
  alt.Y('count(Title)'),
  alt.Color('Category')
)
actions_by_category
```

* based on five topics

```{python}
crim_and_civil=df_21[df_21['Category']=='Criminal and Civil Actions']
crim_and_civil.head()
```

```{python}
def assign_subcategory(title):
    if 'financial' in title.lower():
        return 'Financial Fraud'
    elif 'bank' in title.lower():
        return 'Financial Fraud'
    elif 'embezzled' in title.lower():
        return 'Financial Fraud'
    elif 'doctor' in title.lower():
        return 'Health Care Fraud'
    elif 'nurse' in title.lower():
        return 'Health Care Fraud'
    elif 'hospital' in title.lower():
        return 'Health Care Fraud'
    elif 'drug' in title.lower():
        return 'Drug Enforcement'
    elif 'possession' in title.lower():
        return 'Drug Enforcement'
    elif 'marijuana' in title.lower():
        return 'Drug Enforcement'
    elif 'bribe' in title.lower():
        return 'Bribery/Corruption'
    elif 'favor' in title.lower():
        return 'Bribery/Corruption'
    else:
        return 'Other'

crim_and_civil['Subcategory'] = crim_and_civil['Title'].apply(assign_subcategory)

```

```{python}
#Check that there are some results in each subcategory
crim_and_civil.groupby('Subcategory').size()
```

```{python}
actions_by_subcategory=alt.Chart(crim_and_civil, title='Number of Enforcement Actions by Subcategory over Time').mark_line().encode(
  alt.X('year(Date)'),
  alt.Y('count(Title)'),
  alt.Color('Subcategory')
)
actions_by_subcategory
```
## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
import geopandas as gpd
census_data=gpd.read_file(r"C:\Users\laine\OneDrive\Documents\GitHub\problem-set-5-lauren-and-mohamed\cb_2018_us_state_500k.shp")
```

```{python}
enforcement_actions=df_21[df_21['Category']=='State Enforcement Agencies']
```

```{python}
enforcement_actions.groupby('Agency').size()
```
```{python}
states=['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware',
'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky','Louisiana',
'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska',
'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',
'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas',
'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']
def assign_state(agency):
    agency = agency.lower()  
    for s in states:
        if s.lower() in agency: 
            return s
    return 'Other'

```

```{python}
enforcement_actions['State']=enforcement_actions['Agency'].apply(assign_state)
enforcement_actions.head() 
```

```{python}
census_data=census_data[['NAME', 'geometry']]
merge=enforcement_actions.merge(census_data, left_on='State', right_on='NAME', how='left')
merge=gpd.GeoDataFrame(merge, geometry='geometry')
```

```{python}
count=enforcement_actions.groupby('State').size()
count=count.reset_index()
count.columns=['State', 'Count']
count.head()
merged_counts=count.merge(census_data, left_on='State', right_on='NAME',
how='left')
merged_counts=gpd.GeoDataFrame(merged_counts, geometry='geometry')
```

```{python}
plot_by_state=merged_counts.plot(column='Count', legend=True).set_axis_off()
plot_by_state

```
### 2. Map by District (PARTNER 2)

```{python}
def assign_state(agency):
    for i in enforcement_actions[agency]:
        if i=='X'+"Attorney General":
            return 'X'
enforcement_actions['State']=assign_state('Agency')
enforcement_actions.head()    
```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```